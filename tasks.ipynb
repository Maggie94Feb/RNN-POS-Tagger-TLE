{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-speech tagging for Treebank of Learner English corpora with Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    ">Part-of-speech (POS) tagging is the process of marking up a word in a text (corpus) as corresponding to a particular part of speech, based on both its definition and its context. [Wikipedia](https://en.wikipedia.org/wiki/Part-of-speech_tagging)\n",
    "\n",
    "POS tagging could be the fundamentals of many NLP/NLU tasks, such as Name Entity Recognition (NER) and Abstract Meaning Representation (AMR). In this project, I want to explore the state-of-the-art Recurrent Neural Network (RNN) based models for POS tagging. The following are the candidate models:\n",
    "- Long Short-Term Memory (LSTM)\n",
    "- Bidirectional LSTM (BI-LSTM)\n",
    "- LSTM with a Conditional Random Field (CRF) layer (LSTM-CRF) \n",
    "- Bidirectional LSTM with a CRF layer (BI-LSTM-CRF)\n",
    "\n",
    "(**Update 2018/04/12: the basic LSTM is added**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    ">UD English-ESL/TLE is a collection of 5,124 English as a Second Language (ESL) sentences (97,681 words), manually annotated with POS tags and dependency trees in the Universal Dependencies formalism. Each sentence is annotated both in its original and error corrected forms. The annotations follow the standard English UD guidelines, along with a set of supplementary guidelines for ESL. The dataset represents upper-intermediate level adult English learners from 10 native language backgrounds, with over 500 sentences for each native language. The sentences were randomly drawn from the Cambridge Learner Corpus First Certificate in English (FCE) corpus. The treebank is split randomly to a training set of 4,124 sentences, development set of 500 sentences and a test set of 500 sentences. Further information is available at [esltreebank.org](esltreebank.org). \n",
    "\n",
    "Citation: (Berzak et al., 2016; Yannakoudakis et al., 2011)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader\n",
    "\n",
    "I've built a data loader for this dataset. To use the data loader, you need to first install the [CoNLL-U Parser](https://github.com/EmilStenstrom/conllu) built by [Emil Stenstr√∂m](https://github.com/EmilStenstrom). The following is an example to use data_loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_loader\n",
    "\n",
    "meta_list, data_list = data_loader.load_data(load_train=True, load_dev=True, load_test=True)\n",
    "\n",
    "train_meta, train_meta_corrected, \\\n",
    "dev_meta, dev_meta_corrected, \\\n",
    "test_meta, test_meta_corrected = meta_list\n",
    "\n",
    "train_data, train_data_corrected, \\\n",
    "dev_data, dev_data_corrected, \\\n",
    "test_data, test_data_corrected = data_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata\n",
    "- doc_id: filename (also learner ID) of the original xml file\n",
    "- sent: raw text of the sentence written by the leaner with error corrected tags\n",
    "- native_language: native language of the leaner\n",
    "- age_range: age range of the learner\n",
    "- score: exam score of the learner\n",
    "\n",
    "Some observations:\n",
    "- \"native_language\" enables us to design tasks related to native language identificaiton.\n",
    "- \"age_range\" enables us to identify the learner's age based on his/her writing style.\n",
    "- \"score\" can help us to group learners into categories, such as Beginner, Intermediate, Expert, Fluent, Proficient. It enables us to discover the writing style and common mistakes of different groups of learners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>sent</th>\n",
       "      <th>native_language</th>\n",
       "      <th>age_range</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doc2664</td>\n",
       "      <td>I was &lt;ns type=\"S\"&gt;&lt;i&gt;shoked&lt;/i&gt;&lt;c&gt;shocked&lt;/c&gt;...</td>\n",
       "      <td>Russian</td>\n",
       "      <td>21-25</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doc648</td>\n",
       "      <td>I am very sorry to say it was definitely not a...</td>\n",
       "      <td>French</td>\n",
       "      <td>26-30</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doc1081</td>\n",
       "      <td>Of course, I became aware of her feelings sinc...</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>16-20</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doc724</td>\n",
       "      <td>I also suggest that more plays and films shoul...</td>\n",
       "      <td>Japanese</td>\n",
       "      <td>21-25</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>doc567</td>\n",
       "      <td>Although my parents were very happy &lt;ns type=\"...</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>31-40</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     doc_id                                               sent  \\\n",
       "id                                                               \n",
       "1   doc2664  I was <ns type=\"S\"><i>shoked</i><c>shocked</c>...   \n",
       "2    doc648  I am very sorry to say it was definitely not a...   \n",
       "3   doc1081  Of course, I became aware of her feelings sinc...   \n",
       "4    doc724  I also suggest that more plays and films shoul...   \n",
       "5    doc567  Although my parents were very happy <ns type=\"...   \n",
       "\n",
       "   native_language age_range  score  \n",
       "id                                   \n",
       "1          Russian     21-25   21.0  \n",
       "2           French     26-30   38.0  \n",
       "3          Spanish     16-20   36.0  \n",
       "4         Japanese     21-25   33.0  \n",
       "5          Spanish     31-40   34.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_meta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Format\n",
    "In this project, I will only use \"form\" (words) and \"upostag\" (part-of-speech tags)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>form</th>\n",
       "      <th>lemma</th>\n",
       "      <th>upostag</th>\n",
       "      <th>xpostag</th>\n",
       "      <th>feats</th>\n",
       "      <th>head</th>\n",
       "      <th>deprel</th>\n",
       "      <th>deps</th>\n",
       "      <th>misc</th>\n",
       "      <th>meta_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I</td>\n",
       "      <td>_</td>\n",
       "      <td>PRON</td>\n",
       "      <td>PRP</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>was</td>\n",
       "      <td>_</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBD</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>cop</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>shoked</td>\n",
       "      <td>_</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>JJ</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>root</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>because</td>\n",
       "      <td>_</td>\n",
       "      <td>SCONJ</td>\n",
       "      <td>IN</td>\n",
       "      <td>None</td>\n",
       "      <td>8</td>\n",
       "      <td>mark</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I</td>\n",
       "      <td>_</td>\n",
       "      <td>PRON</td>\n",
       "      <td>PRP</td>\n",
       "      <td>None</td>\n",
       "      <td>8</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>had</td>\n",
       "      <td>_</td>\n",
       "      <td>AUX</td>\n",
       "      <td>VBD</td>\n",
       "      <td>None</td>\n",
       "      <td>8</td>\n",
       "      <td>aux</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>alredy</td>\n",
       "      <td>_</td>\n",
       "      <td>ADV</td>\n",
       "      <td>RB</td>\n",
       "      <td>None</td>\n",
       "      <td>8</td>\n",
       "      <td>advmod</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spoken</td>\n",
       "      <td>_</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBN</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>advcl</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>with</td>\n",
       "      <td>_</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>case</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>them</td>\n",
       "      <td>_</td>\n",
       "      <td>PRON</td>\n",
       "      <td>PRP</td>\n",
       "      <td>None</td>\n",
       "      <td>8</td>\n",
       "      <td>nmod</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>and</td>\n",
       "      <td>_</td>\n",
       "      <td>CONJ</td>\n",
       "      <td>CC</td>\n",
       "      <td>None</td>\n",
       "      <td>8</td>\n",
       "      <td>cc</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>I</td>\n",
       "      <td>_</td>\n",
       "      <td>PRON</td>\n",
       "      <td>PRP</td>\n",
       "      <td>None</td>\n",
       "      <td>14</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>had</td>\n",
       "      <td>_</td>\n",
       "      <td>AUX</td>\n",
       "      <td>VBD</td>\n",
       "      <td>None</td>\n",
       "      <td>14</td>\n",
       "      <td>aux</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>taken</td>\n",
       "      <td>_</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBN</td>\n",
       "      <td>None</td>\n",
       "      <td>8</td>\n",
       "      <td>conj</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>two</td>\n",
       "      <td>_</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "      <td>None</td>\n",
       "      <td>16</td>\n",
       "      <td>nummod</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>autographs</td>\n",
       "      <td>_</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NNS</td>\n",
       "      <td>None</td>\n",
       "      <td>14</td>\n",
       "      <td>dobj</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>.</td>\n",
       "      <td>_</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>.</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>punct</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          form lemma upostag xpostag feats  head  deprel  deps  misc  meta_id\n",
       "id                                                                           \n",
       "1            I     _    PRON     PRP  None     3   nsubj  None  None        1\n",
       "2          was     _    VERB     VBD  None     3     cop  None  None        1\n",
       "3       shoked     _     ADJ      JJ  None     0    root  None  None        1\n",
       "4      because     _   SCONJ      IN  None     8    mark  None  None        1\n",
       "5            I     _    PRON     PRP  None     8   nsubj  None  None        1\n",
       "6          had     _     AUX     VBD  None     8     aux  None  None        1\n",
       "7       alredy     _     ADV      RB  None     8  advmod  None  None        1\n",
       "8       spoken     _    VERB     VBN  None     3   advcl  None  None        1\n",
       "9         with     _     ADP      IN  None    10    case  None  None        1\n",
       "10        them     _    PRON     PRP  None     8    nmod  None  None        1\n",
       "11         and     _    CONJ      CC  None     8      cc  None  None        1\n",
       "12           I     _    PRON     PRP  None    14   nsubj  None  None        1\n",
       "13         had     _     AUX     VBD  None    14     aux  None  None        1\n",
       "14       taken     _    VERB     VBN  None     8    conj  None  None        1\n",
       "15         two     _     NUM      CD  None    16  nummod  None  None        1\n",
       "16  autographs     _    NOUN     NNS  None    14    dobj  None  None        1\n",
       "17           .     _   PUNCT       .  None     3   punct  None  None        1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Continuous POS tagging with RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture\n",
    "\n",
    "In this task, a POS tagger was trained with all train data (4124 sentences), validated with dev data (500 sentences), and tested with test data (500 sentences). The following is the architecture:\n",
    "\n",
    "![Task1 Architecture](figures/task1-arch.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Models\n",
    "\n",
    "In this project, I mainly use [PyTorch](http://pytorch.org/) to implement the RNN models. The following are what I've already implemented:\n",
    "\n",
    "#### Long Short-Term Memory (LSTM)\n",
    ">Long short-term memory (LSTM) units (or blocks) are a building unit for layers of a recurrent neural network (RNN). A RNN composed of LSTM units is often called an LSTM network. A common LSTM unit is composed of a cell, an input gate, an output gate and a forget gate. The cell is responsible for \"remembering\" values over arbitrary time intervals; hence the word \"memory\" in LSTM. [Wikepedia](https://en.wikipedia.org/wiki/Long_short-term_memory)\n",
    "\n",
    "The following is the high-level architecture for the LSTM model:\n",
    "\n",
    "![Task1_Feature](figures/task1-w2v-lstm.png)\n",
    "\n",
    "#### Word Features\n",
    "\n",
    "I use the pre-trained [Word2Vec model](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit) built with Google News corpus (3 million 300-dimension English word vectors). Although it might not be the best choice (e.g. Google News corpus might not be representative for the English Learner text), it's still a legitimate choice: 1) It saves my time to build a large dictionary which cover all words in the UD English-ESL/TLE corpus; 2) It saves my time and computing resources to build large/sparse unigram vectors for words, and I don't need to worry about dimension reduction for now; 3) 300-dim w2v vector is small enough for this task, and the dimension is fixed so the vector can be directly used in NN. 4) It's free and available on Google Drive :). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tuning\n",
    "\n",
    "#### Number of epochs\n",
    "\n",
    "The dataset was divided into train, dev, test sets. I used train and dev sets to observe the fluctuation of accuracy and loss during the training process of 1000 epochs. There are 17 different POS tags in this experiment. The prediction is considered correct only if it is the same as the actual POS tag. At the end of the 1000 epochs, the LSTM model achieves **88.09%** training accuracy, **80.9%** validation accuracy, and **88.42%** test accuracy. According to the following figures, there is no apparent overfitting, and the best number of training epoch is around 650-700. However, the intersection between train loss line and dev loss line was not shown in the experiment.\n",
    "\n",
    "![Task1_Accu](figures/accu_linear.png)\n",
    "![Task1_Loss](figures/loss_linear.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Berzak, Y., Kenney, J., Spadine, C., Wang, J. X., Lam, L., Mori, K. S., ... & Katz, B. (2016). Universal dependencies for learner English. arXiv preprint arXiv:1605.04278.\n",
    "2. Yannakoudakis, H., Briscoe, T., & Medlock, B. (2011, June). A new dataset and method for automatically grading ESOL texts. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1 (pp. 180-189). Association for Computational Linguistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
